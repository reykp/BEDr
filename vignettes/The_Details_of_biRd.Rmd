---
title: "The Details of biRd"
author: "Kathleen P. Rey"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    fig_width: 8
    fig_height: 4
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(eval = FALSE)


# library(ggplot2)
# library(dplyr)
# library(tidyr)
# library(gridExtra)
#library(biRd)
```

## Introduction

Bayesian!

Nonparametric!!!!

Ease of implementation!!!

## Methodology

### DUOS

#### Introduction

The Bayesian density estimator in this section is designed to have little to know input necessary from the user. Although, knowledge of the complexity of the density and the size of the data can definitley and should influence choices in the input, this method can make reasonable choices without user input other than the data. This method is a histogram-like density estimate, but rather than the widths being fixed or the widths varying, but the midpoints being fixed, the end points of the bins are random. The number of bins is chosen by the user, or as seen later, can be left to the default.

The positions of the bin end points (or cut-points as they are refered to throughout the vignette) are random order statistics on unif(0,1). From here on, this method will be refered to as DUOS: Distribution of Uniform Order Statistics.

Before going into detail, an example of what `duos` is creating is below. The resultin gposterior mean density (in red) is a mean of many different step functions (in blue). These are both0 overalayed over a histogram of the actual data. 
 
```{r, echo = FALSE}

# Read in sample data
y <- read.csv("Data/Data_Vshape_Size300_Rep4.csv")
y <- y$x

# Cut- points from a set of test data
C <- read.csv("Data/C_Vshape_300_c7_Rep4.csv", header=TRUE)
# Bin proporitions from the same set of test data
P <- read.csv("Data/P_Vshape_300_c7_Rep4.csv", header=TRUE)

# Remove the first column with is row numbers
C <- C[,-1]
P <- P[,-1]

# Set the burnin so not plotting too much output
Burnin <- 19800
# Set the number of cut-points to a global parameters
k <<- ncol(C)

# Create a grid to estimate the densities on
input <<- seq(0.01,.99, by=.01)

# Pdf function
pdf <- function(x){
    pi <- rep(0,length(input))
    c_full <- c(0, x[1:k],1)
    p <- x[{k+1}:length(x)]
    for (j in 1:(k+1)){
      pi[which(input<c_full[(j+1)] & input>=c_full[j])]<- p[j]/(c_full[(j+1)]-c_full[j])
    }
    return(pi)
  }

# Get subsets of iterations
C_sub <- C[Burnin:nrow(C),]
P_sub <- P[Burnin:nrow(C),]

# Estimate the density at each iteration 
pdf_y_matrix <- apply(cbind(C_sub,P_sub), 1, pdf)

# Data to plot
pdf_plot <- data.frame(X=input, pdf_y_matrix)

# Stack data
pdf_plot <- pdf_plot %>% gather(Run, Density, -X) %>% filter(Density<5)

# Data to use as histogram
j_hist <- data.frame(y=y)

# Calculate the mean density estimate
pdf_mean <- data.frame(X=input, PDF= apply(pdf_y_matrix, 1, mean))

ggplot(data=j_hist, aes(y))+
  geom_line(data=pdf_plot, aes(X, Density,group=Run, color="blue"),alpha=.9)+
    geom_histogram(aes(y=..density..),fill="grey", color="black",alpha=.9)+
  geom_line(data=pdf_mean, aes(X, PDF, color="red"), size=1)+
  theme_bw()+ylab("Posterior Mean Density")+xlab("X")+
  theme(text = element_text(size=15),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  scale_colour_manual(name = '',
  values =c('blue'='blue','red'='red'), labels = c('Single Iteration','Posterior Mean'))

```

The particular results from above were simulated from a distribution that lies between 0 and 1. `duos` is designed to work on data that is in the range (0,1). If the data is not between 0 and 1, is is standardize using the options in section [Density Estimation](#duos-density-estimation). As mentioned above, the data model is a step function as specified below:

* Let $k$ = the number of cut-points.
* Let $\gamma_j$ represent a cut-point: j = 1,..k.
* Let $\pi_j$ represent the proportion associated with the bin ($\gamma_{j-1}, \gamma_j$)

$f_{\boldsymbol\gamma,\boldsymbol\pi}(x)$ = \left\
        \begin{array}{ll}
            \frac{\pi_1}{(\gamma_1)} & \quad 0 \leq x  < \gamma_1 \\
            \frac{\pi_2}{(\gamma_2-\gamma_1)} & \quad \gamma_1 \leq x  < \gamma_2 \\
            \frac{\pi_3}{(\gamma_3-\gamma_2)} & \quad \gamma_2 \leq x  < \gamma_3 \\
            ... & \quad ... \leq x  < ... \\
            \frac{\pi_k}{(\gamma_k-\gamma_{k-1})} & \quad \gamma_{k-1} \leq x  < \gamma_k \\
            \frac{\pi_{k+1}}{(1-\gamma_k)} & \quad \gamma_k \leq x  < 1 \\
        \end{array}

$k$ : the number of cut points \newline
\indent
$\boldsymbol\gamma : 0 < \gamma_1 < \gamma_2 < ... < \gamma_{k-1} < \gamma_k < 1$ \newline
\indent
$\boldsymbol\pi : \pi_k \geq 0$ and  $\sum_{j=1}^{k+1} \pi_j = 1$

$\boldsymbol\gamma$ and $\boldsymbol\pi$ are the unknown parameters and therefore, require priors.

Assume $boldsymbol\gamma$ and  $boldsymbol\pi$ are independent (i.e. $p(\boldsymbol\gamma, \boldsymbol\pi) = p(\boldsymbol\gamma)p(\boldsymbol\pi)$).

**Priors:**

$\boldsymbol\gamma \sim$ *DUOS prior*

i.e. Draw k values from $unif(0,1)$ and order: $0 < \gamma_1 < \gamma_2 < ... < \gamma_{k-1} < \gamma_k < 1$.

$\boldsymbol\pi \sim Dir(\boldsymbol\alpha)$

where $\alpha_j = 1$ for $j = 1,2,...,k,k+1$ 

Noninformative priors were chosen so that little informational input is required from the user.

Let $x_1, x_2,...,x_n$ be independent and identical samples from some unknown distribution.

`duos` implements a Gibbs algorithm using the following full conditionals:

$p(\boldsymbol\pi|\textbf x,\boldsymbol\gamma) \sim Dir(\alpha^*)$

where $\alpha^* =(1+\sum_{i=1}^{n} \mathrm{I}(0\leq x_i<\gamma_1), 1+\sum_{i=1}^{n} \mathrm{I}(\gamma_1\leq x_i<\gamma_2),..., 1+\sum_{i=1}^{n}\mathrm{I}(\gamma_k\leq x_i<1))$

Note the meaning of the notation $\boldsymbol\gamma_{-j}$ used below. This indicates the $\gamma_j$ is conditioning on all other $\gamma_j$'s except $\gamma_j$. Thus, the full conditional for each $\gamma_j$ is as follows:

$p(\gamma_j|\textbf x,\boldsymbol\pi,\boldsymbol\gamma_{-j}) \propto {\frac{\pi_j}{\gamma_j-\gamma_{j-1}}}^{\sum_{i=1}^{n}\mathrm{I}(\gamma_{j-1}\leq x_i<\gamma_j)}\frac{\pi_{j+1}}{\gamma_{j+1}-\gamma_j}^{\sum_{i=1}^{n}\mathrm{I}(\gamma_j \leq x_i<\gamma_{j+1})}$
where $\gamma_j \in [\gamma_{j-1}, \gamma_{j+1})$

Although the full conditional for the bin probability parameters is a known distribution, the full conditional for the cut-point parameters is not.  

```{r, echo = FALSE}
# Code to creatae plot of what a full conditional distribution for a cut-point might look like
# w is cut-point, row is with row want to create plot from, and nsim is how many values to calculate kernel at 

w <- 3
row <- 15000
nsim <- 1000

# Read in data sample
y <- read.csv("Data/Data_Vshape_Size300_Rep4.csv")
ys <- y$x

# Read in cut-points and bin probabilities
C <- read.csv("Data/C_Vshape_300_c7_Rep4.csv", header=TRUE)
P <- read.csv("Data/P_Vshape_300_c7_Rep4.csv", header=TRUE)
# Remove first column that contains row numberes
C <- C[,-1]
P <- P[,-1]

# Get cut-points
c <- as.numeric(C[row,])
# Add in - and 1
Q_C_extra <- c(0, c, 1)
# index parameter
w_p2 <- w+2
# Get data between bordering cut-points
xm <- c(Q_C_extra[w], ys[ys>=Q_C_extra[w]&ys<Q_C_extra[{w+2}]], Q_C_extra[{w+2}])
# find number of observatiosn between C[w-1] and C[w+1]
m <- length(xm)-2

# Initialize vector to contains maximums on each interval
c_max <- NA
# Initialize vector to contain minimums on each interval
# Get for first interval in the vector
c_max_choices <- c(xm[1], xm[2])
# The maximum from derivatives is Q_C_extra[w] which is xm[1]
# Pairs (c(1/2), c(1/3), c(2/3))
# Create to contain ratios of the pdf's

# option1/option2
c_max_maxes <- (Q_C_extra[w_p2]-c_max_choices[1])^(-m)/((Q_C_extra[w_p2]-c_max_choices[2])^(-m))

# Compare ratio to 1 to find maximum
if (c_max_maxes>1){
  c_max[1] <- c_max_choices[1]
} else{
  c_max[1] <- c_max_choices[2]
}

# If more than one observation between cut-points
if (m >1){

  for (q in 2:m){


    q_p1 <- q+1
    # Counter for data as upper bound, negative since power for denominator

    m1_p1 <- -q+1
    mm_pq_m1 <- -m+q-1
    # Choices for maximum
    c_max_choices <- c(xm[q], xm[q_p1], (((m-q+1)*Q_C_extra[w]+(q-1)*Q_C_extra[w_p2])/m))

    # Pairs (c(1/2), c(1/3), c(2/3))
    c_max_maxes <- NA
    # option1/option2
    c_max_maxes[1] <- exp(m1_p1*log(c_max_choices[1]-Q_C_extra[w])+mm_pq_m1*log(Q_C_extra[w_p2]-c_max_choices[1])-m1_p1*log(c_max_choices[2]-Q_C_extra[w])-mm_pq_m1*log(Q_C_extra[w_p2]-c_max_choices[2]))
    # Option1/option3
    c_max_maxes[2] <- exp(m1_p1*log(c_max_choices[1]-Q_C_extra[w])+mm_pq_m1*log(Q_C_extra[w_p2]-c_max_choices[1])-m1_p1*log(c_max_choices[3]-Q_C_extra[w])-mm_pq_m1*log(Q_C_extra[w_p2]-c_max_choices[3]))
    # Option2/option3
    c_max_maxes[3] <- exp(m1_p1*log(c_max_choices[2]-Q_C_extra[w])+mm_pq_m1*log(Q_C_extra[w_p2]-c_max_choices[2])-m1_p1*log(c_max_choices[3]-Q_C_extra[w])-mm_pq_m1*log(Q_C_extra[w_p2]-c_max_choices[3]))

    # Compare ratio to 1 to find maximum
    if (c_max_choices[3] >= xm[q] & c_max_choices[3] < xm[q_p1]){

      if (c_max_maxes[1] > 1 & c_max_maxes[2] > 1) {

        c_max[q] <- c_max_choices[1]
      } else if (c_max_maxes[1] < 1 & c_max_maxes[3] > 1) {

        c_max[q] <- c_max_choices[2]
      } else {
        c_max[q] <- c_max_choices[3]
      }
    } else if ( c_max_maxes[1] > 1) {

      c_max[q] <- c_max_choices[1]
    } else {
      c_max[q] <- c_max_choices[2]
    }


  }
}

# GET for last
c_max_choices <- c(xm[m+1], xm[m+2])

#  Pairs (c(1/2), c(1/3), c(2/3))
#option1/option2
c_max_maxes <- (c_max_choices[1]-Q_C_extra[w])^(-m)/((c_max_choices[2]-Q_C_extra[w])^(-m))

if (c_max_maxes[1] > 1){
  c_max[m+1] <- c_max_choices[1]
} else {
  c_max[m+1] <- c_max_choices[2]
}

# Get bin probabilities
p <- P[row,]
# Values at which to calculate kernel
x <- runif(nsim, c[{w-1}], c[{w+1}])

# Calculate the kernel of the density
pdf_kernel <- NA
for (i in 1:length(x)){
  n1 <- length(which(ys>=c[{w-1}]&ys<x[i]))
  n2 <- length(which(ys<c[{w+1}]&ys>=x[i]))

  pdf_kernel[i] <- (p[w]/(x[i]-c[{w-1}]))^n1*(p[{w+1}]/(c[{w+1}]-x[i]))^n2
   #print(i)
}

# Calculate what bounding function would look like
bound_plot <- NA
for (i in 1:{length(xm)-1}){
  n1 <- i-1
  n2 <- length(xm)-2-i+1

  bound_plot[which(x>=xm[i]&x<xm[{i+1}])] <- (p[w]/(c_max[i]-c[{w-1}]))^n1*(p[{w+1}]/(c[{w+1}]-c_max[i]))^n2
}

# Make and sort data frame
d2 <- data.frame(x,pdf_kernel, bound_plot)
d2 <- d2 %>% arrange(x)

# Create data set to identify discontinuities in graph
locations <- NA
index <- 1
for (i in 1:{length(xm)-1}){
  if (length(which(d2$x>=xm[{i+1}])>0)){
    locations[index] <- min(which(d2$x>=xm[{i+1}]))
    index <- index+1
  }
}

d2$disc <- "Cont"
d2$disc[locations] <- "Disc"

options(digits=4)

ggplot(d2)+geom_point(aes(x,pdf_kernel, shape=disc, size=disc))+
  theme_bw()+xlab("x")+ylab("Full Conditional Kernel")+
  theme(text = element_text(size=15),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        legend.position = "none")+
  scale_shape_manual(values=c(16,1))+
  scale_size_manual(values=c(1,4))

```

Note the open circles where discontinuities exist. The breaks in the kernel of the full conditional for a cut-point occur at the data points. This is because, between two data points from the sample, the counts in the powers in the full conditional are constant. 

Given the complecated nature of this density, it is numerically difficult to find the normalizing constant, not to mention simulating from this distribution. Thus, in order to obtain samples from this distribution, rejection sampling was implemented. The proposal (and boudning) distribution used is depicted in red in the graph below.

```{r, echo = FALSE}
ggplot(d2)+geom_point(aes(x,pdf_kernel, shape=disc, size=disc))+
  geom_point(aes(x, bound_plot), color="red",size=1.5)+
  theme_bw()+xlab("x")+ylab("Kernel")+
  theme(text = element_text(size=20),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        legend.position = "none")+
  scale_shape_manual(values=c(16,1))+
  scale_size_manual(values=c(1,4))

```

### GOLD

## R Package: biRD (Bayesian Implimention in R for Density estimation)

Demonstrations of the functions for `duos` and `gold` are demonstrated on a wide variety of distributions. These are depicted in the plot below.


```{r, echo = FALSE}
# Grid to estimate densities on
x <- seq(from=.001, to=.999, by=.001)
# Unif
Uniform <- rep(1, length(x))

# Exponential
Exponential <- dexp(x, 1)

# Arcsin
Arcsin <- dbeta(x, .5,.5)

# Beta(2,5)
Beta <- dbeta(x, 2,5)

# Jagged
Jagged <- NA
Jagged[x>=0 & x <.2] <- 1.5-5*x[x>=0 & x<.2]
Jagged[x>=.2 & x <.4] <- -0.5+5*x[x>=.2 & x<.4]
Jagged[x>=.4 & x <.6] <- 3.5-5*x[x>=.4 & x<.6]
Jagged[x>=.6 & x <.8] <- -2.5+5*x[x>=.6 & x<.8]
Jagged[x>=.8 & x <1] <- 5.5-5*x[x>=.8 & x<1]

# Normal
y <- x*(3.999+3.999)+-3.999
Normal <- dnorm(y)

# Bimodal
# Sample from uniform
u <- runif(100000)

# Variable to store the samples from the mixture distribution
rs <- rep(NA,100000)

#Sampling from the mixture
for(i in 1:100000){
  if(u[i]<.3){
    rs[i] <- rnorm(1,0,1)
  }else {
    rs[i] <- rnorm(1,4,1)
  }
}
# Rescale
y <- x*(max(rs)-min(rs))+min(rs)
Bimodal <- .3*dnorm(y,0,1) + .7*dnorm(y,4,1)

# Claw
# Sample from uniform
u <- runif(100000)

# Variable to store the samples from the mixture distribution
rs <- rep(NA,100000)

#Sampling from the mixture
for(i in 1:100000){
  if(u[i]<.5){
    rs[i] <- rnorm(1,0,1)
  }else if(u[i]<.6){
    rs[i] <- rnorm(1,-1,.1)
  }else if (u[i] < .7){
    rs[i] <- rnorm(1,-.5,.1)
  }else if (u[i]<.8){
    rs[i] <- rnorm(1,0,.1)
  }else if (u[i] < .9){
    rs[i] <- rnorm(1,.5,.1)
  }else {
    rs[i] <- rnorm(1,1,.1)
  }
}

# Rescale
y <- x*(max(rs)-min(rs))+min(rs)
Claw <- .5*dnorm(y)+.1*dnorm(y,-1,.1)+.1*dnorm(y,-.5,.1)+.1*dnorm(y,0,.1)+.1*dnorm(y,.5,.1)+.1*dnorm(y,1,.1)

# Trimodal
# Sample from uniform
u <- runif(100000)
# Variable to store the samples from the mixture distribution
rs <- rep(NA,100000)

# Sampling from the mixture
for(i in 1:100000){
  if(u[i]<.2){
    rs[i] <- rnorm(1,0,1)
  }else if(u[i]<.5){
    rs[i] <- rnorm(1,6,1)
  }else{
    rs[i] <- rnorm(1,2,.1)
  }
}

y <- x*(max(rs)-min(rs))+min(rs)
Trimodal = .2*dnorm(y,0,1) + .5*dnorm(y,6,1) + .3*dnorm(y,2,.1)

distributions <- data.frame(x, Uniform, Arcsin, Beta, Exponential, Jagged, Normal, Bimodal,Trimodal, Claw)

d_melt <- gather(distributions, variable, value, -x)
d_melt$variable <- factor(d_melt$variable, levels=c("Uniform", "Beta","Normal","Exponential", "Arcsin",
                                                    "Bimodal","Jagged", "Trimodal","Claw"))

ggplot(d_melt, aes(x, value))+geom_line(size=1,color="dodgerblue4")+
  facet_wrap(~variable, nrow=3, scales="free_y")+
  theme_bw()+
  theme(text = element_text(size=25),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        legend.position = "none")
  #scale_color_viridis(discrete=TRUE,option="magma",end=.8)
  #scale_color_brewer(palette = "Set1")
```

### DUOS

The package is desinged to:
* Run Gibbs algorithm to sample from the posterior distributions for the cut-points and bin probabilities
* Provide useful convergence diagnostic plots
* Plot the Bayesian estimate of the PDF and CDF
* Give estimates of the PDF and CDF at new values
* Estimate a variety of statistics 

All of the functionally of the package is demonstrated by walking through the analysis of several sets of simulated data.

#### Density Estimation {#duos-density-estimation}

The code below creates data from each of the nine densitites described above. For the purposes of demonstration, a sample set of size 100 is used for proceed with most results. However, plots will be used to demonstrate overall how the estimate changes as the sample size increases. The plot below shows the histograms of the sampled data.

```{r, echo = FALSE}
# # Sample size
# n <- 100
# 
# # Uniform
# y_unif <-  runif(n, 0, 1)
# y_unif_500 <- runif(500, 0, 1)
# y_unif_50 <- runif(50, 0, 1)
# # Beta
# y_beta <- rbeta(n, 2, 5)
# 
# # Normal
# y_norm <- rnorm(n, 0, 1)
# 
# # Exponential
# y_exp <- rexp(n, 1)
# 
# # Arcsin
# y_arcsin <- rbeta(n, 0.5, 0.5)
# 
# y_arcsin_50 <- rbeta(50, 0.5, 0.5)
# y_arcsin_500 <- rbeta(500, 0.5, 0.5)
# # Bimodal
#   # Sample from uniform
#   u <- runif(n)
#   
#   # Variable to store data                                        
#   y_bimodal <- rep(NA,n)
#   
#   # Sampling from the mixture
#   for(i in 1:n){
#       if(u[i]<.3){
#         y_bimodal[i] <- rnorm(1, 0, 1)
#       }else {
#         y_bimodal[i] <- rnorm(1, 4, 1)
#       }
#   }
# 
# # Jagged
#   y1 = 1.5
#   y2 = .5
#     
#   m1 = (y2-y1)/.2
#   m2 = (y1-y2)/.2
#   m3 = (y2-y1)/.2
#   m4 = (y1-y2)/.2
#   m5 = (y2-y1)/.2
#   
#   b1 = y1
#   b2 = 2*y2-y1
#   b3 = 3*y1-2*y2
#   b4 = 4*y2-3*y1
#   b5 = 5*y1-4*y2
#     
#   x_jag = runif(n, 0, 1)
#     
#   y_jagged <- NA
#   y_jagged[x_jag>=0 & x_jag<.2] <- (-1.5+sqrt(1.5^2-10*x_jag[x_jag>=0 & x_jag<.2]))/(-5)
#   y_jagged[x_jag>=.2 & x_jag<.4] <- (.5+sqrt(.5^2-4*(5/2)*(.2-x_jag[x_jag>=.2 & x_jag<.4])))/(5)
#   y_jagged[x_jag>=.4 & x_jag<.6] <- (-3.5+sqrt(3.5^2-4*(-5/2)*(-.6-x_jag[x_jag>=.4 & x_jag<.6])))/(-5)
#   y_jagged[x_jag>=.6 & x_jag<.8] <- (2.5+sqrt(2.5^2-4*(5/2)*(1.2-x_jag[x_jag>=.6 & x_jag<.8])))/(5)
#   y_jagged[x_jag>=.8 & x_jag<1] <- (-5.5+sqrt(5.5^2-4*(-5/2)*(-2-x_jag[x_jag>=.8 & x_jag<1])))/(-5)
  # x_jag = runif(350, 0, 1)
  # y_jagged_350 <- NA
  # y_jagged_350[x_jag>=0 & x_jag<.2] <- (-1.5+sqrt(1.5^2-10*x_jag[x_jag>=0 & x_jag<.2]))/(-5)
  # y_jagged_350[x_jag>=.2 & x_jag<.4] <- (.5+sqrt(.5^2-4*(5/2)*(.2-x_jag[x_jag>=.2 & x_jag<.4])))/(5)
  # y_jagged_350[x_jag>=.4 & x_jag<.6] <- (-3.5+sqrt(3.5^2-4*(-5/2)*(-.6-x_jag[x_jag>=.4 & x_jag<.6])))/(-5)
  # y_jagged_350[x_jag>=.6 & x_jag<.8] <- (2.5+sqrt(2.5^2-4*(5/2)*(1.2-x_jag[x_jag>=.6 & x_jag<.8])))/(5)
  # y_jagged_350[x_jag>=.8 & x_jag<1] <- (-5.5+sqrt(5.5^2-4*(-5/2)*(-2-x_jag[x_jag>=.8 & x_jag<1])))/(-5)
#   
#   
#   # Trimodal
#     u <- runif(n)
#                                          
#     y_trimodal <- rep(NA,n)
#     
#     #Sampling from the mixture
#     for(i in 1:n){
#       if(u[i]<.2){
#         y_trimodal[i] <- rnorm(1, 0, 1)
#       }else if(u[i]<.5){
#         y_trimodal[i] <- rnorm(1, 6, 1)
#       }else{
#         y_trimodal[i] <- rnorm(1, 2, 0.1)
#       }
#     }
  # # Trimodal
  #   u <- runif(200)
  # 
  #   y_trimodal_200 <- rep(NA,200)
  # 
  #   #Sampling from the mixture
  #   for(i in 1:200){
  #     if(u[i]<.2){
  #       y_trimodal_200[i] <- rnorm(1, 0, 1)
  #     }else if(u[i]<.5){
  #       y_trimodal_200[i] <- rnorm(1, 6, 1)
  #     }else{
  #       y_trimodal_200[i] <- rnorm(1, 2, 0.1)
  #     }
  #   }
  # 
#   
# # Claw
#     u <- runif( n)
#     
#     y_claw = rep(NA, n)
#     
#     # Sampling from the mixture
#     for(i in 1: n){
#       if(u[i]<.5){
#         y_claw[i] <- rnorm(1, 0, 1)
#       }else if(u[i]<.6){
#         y_claw[i] <- rnorm(1, -1, 0.1)
#       }else if (u[i] < .7){
#         y_claw[i] <- rnorm(1, -0.5, 0.1)
#       }else if (u[i]<.8){
#         y_claw[i] <- rnorm(1, 0, 0.1)
#       }else if (u[i] < .9){
#         y_claw[i] <- rnorm(1, 0.5, 0.1)
#       }else {
#         y_claw[i] <- rnorm(1, 1, 0.1)
#       }
#     }
# Claw
    # u <- runif(400)
    # 
    # y_claw_400 = rep(NA, 400)
    # 
    # # Sampling from the mixture
    # for(i in 1:400){
    #   if(u[i]<.5){
    #     y_claw_400[i] <- rnorm(1, 0, 1)
    #   }else if(u[i]<.6){
    #     y_claw_400[i] <- rnorm(1, -1, 0.1)
    #   }else if (u[i] < .7){
    #     y_claw_400[i] <- rnorm(1, -0.5, 0.1)
    #   }else if (u[i]<.8){
    #     y_claw_400[i] <- rnorm(1, 0, 0.1)
    #   }else if (u[i] < .9){
    #     y_claw_400[i] <- rnorm(1, 0.5, 0.1)
    #   }else {
    #     y_claw_400[i] <- rnorm(1, 1, 0.1)
    #   }
    # }
# 
#     
# distributions <- data.frame(Uniform = y_unif, Beta = y_beta, Normal= y_norm, Exponential = y_exp, Arcsin = y_arcsin, Bimodal = y_bimodal, Jagged = y_jagged, Trimodal = y_trimodal,Claw = y_claw)
# 
# distr_gather <- gather(distributions, variable, value)
# distr_gather$variable <- factor(distr_gather$variable, levels=c("Uniform", "Beta","Normal","Exponential",
#                                                                 "Arcsin","Bimodal","Jagged", "Trimodal","Claw"))

distr_gather <- read.csv("Data/distr_gather.csv")


 distr_gather$variable <- factor(distr_gather$variable, levels=c("Uniform", "Beta","Normal","Exponential", "Arcsin","Bimodal","Jagged","Trimodal","Claw"))

 ggplot(distr_gather, aes(value))+geom_histogram(color="black", fill = "grey", bins = 15)+
  facet_wrap(~variable, nrow=3, scales="free")+
  theme_bw()+
  theme(text = element_text(size=20),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        legend.position = "none")

y_unif <- distr_gather$value[distr_gather$variable=="Uniform"]
y_beta <- distr_gather$value[distr_gather$variable=="Beta"]
y_normal <- distr_gather$value[distr_gather$variable=="Normal"]
y_exp <- distr_gather$value[distr_gather$variable=="Exponential"]
y_arcsin <- distr_gather$value[distr_gather$variable=="Arcsin"]
y_bimodal <- distr_gather$value[distr_gather$variable=="Bimodal"]
y_jagged <- distr_gather$value[distr_gather$variable=="Jagged"]
y_trimodal <- distr_gather$value[distr_gather$variable=="Trimodal"]
y_claw <- distr_gather$value[distr_gather$variable=="Claw"]

y_claw_400 <- read.csv("Data/y_claw_400.csv")[,1]
y_unif_50 <- read.csv("Data/y_unif_50.csv")[,1]
y_unif_500 <- read.csv("Data/y_unif_500.csv")[,1]
y_arcsin_500 <- read.csv("Data/y_arcsin_500.csv")[,1]
y_jagged_350 <- read.csv("Data/y_jagged_350.csv")[,1]
y_unif_500 <- read.csv("Data/y_unif_500.csv")[,1]
y_arcsin_50 <- read.csv("Data/y_arcsin_50.csv")[,1]
y_trimodal_200 <- read.csv("Data/y_trimodal_200.csv")[,1]

g1 <- ggplot(data.frame(y_unif_50), aes(y_unif_50))+geom_histogram(color="black", fill = "grey", bins = 20)+
  theme_bw()+
  theme(text = element_text(size=20),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        legend.position = "none")+ggtitle("Uniform: n = 50")

g4 <- ggplot(data.frame(y_claw_400), aes(y_claw_400))+geom_histogram(color="black", fill = "grey", bins = 20)+
  theme_bw()+
  theme(text = element_text(size=20),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        legend.position = "none")+ggtitle("Claw: n = 400")

g2 <- ggplot(data.frame(y_normal), aes(y_normal))+geom_histogram(color="black", fill = "grey", bins = 20)+
  theme_bw()+
  theme(text = element_text(size=20),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        legend.position = "none")+ggtitle("Normal: n = 100")

g3 <- ggplot(data.frame(y_trimodal_200), aes(y_trimodal_200))+geom_histogram(color="black", fill = "grey", bins = 20)+
  theme_bw()+
  theme(text = element_text(size=20),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        legend.position = "none")+ggtitle("Trimodal: n = 200")

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_DuosTest.jpg", width=1094, height=541)
grid.arrange(g1, g2, g3, g4)
dev.off()

```

The function to start with is `duos`. This is the function that runs the actual Gibbs algorithm. Prior values are set in this function as well as the number of iterations. 
`y`:

```{r}
# Demonstrate error if enter y with missing values
y <- c(rnorm(50), NA)
duos(y = y)
```
`k`: 

This is the number of cut-points to have `duos` use. If k cut-points are chosen, there are k+1 bins. \textbf{More cut-points} are needs as the sample size increases. Typically, this is an additional cut-point for each additional 50 data points. It is recommended that you start with at least $k = 3$. This default is incorporated in to the function. Note that a counter is printed every 1000 iterations to notify you on the process of `duos`.


```{r, eval=FALSE}
# If you run 'duos' with all defaults, it automatically chooses the 
# number of cut-points based on the sample size. 
duos_unif_50 <- duos(y = y_unif_50)
# Since y_unif_50 has 50 data points in it, the default is 4 cut-points. 
# Use the dollar sign to print the 'k' from the 'duos' output.
duos_unif_50$k
```

```{r, echo=FALSE}
load("R Objects/duos_unif_50.RData")
```

```{r}
duos_unif_50$k

```

However, for some densities, more cut-points might be needed based on prior knowledge of the data.

```{r, eval = FALSE}
# The default for a data set of size 400 is 11, but more might be needed
# given the complexity of the distribution. 
# Specify 15 cut-points.
duos_claw_400 <- duos(y = y_claw_400, k = 15)

duos_claw_400_N <- duos(y = y_claw_400, k = 15, N = 50000)
#save(duos_claw_400, file = "R Objects/duos_claw_400.csv")

duos_claw_400_alt <- duos(y = y_claw_400, k = 15, scale_l = sd(y_claw_400), scale_u = sd(y_claw_400))

```


```{r, echo=FALSE}
load("R Objects/duos_claw_400.RData")
load("R Objects/duos_claw_400_N.RData")

```

```{r, eval = FALSE}
duos_exp <- duos(y = y_exp, scale_u = sd(y_exp))
save(duos_exp, file = "R Objects/duos_exp.RData")
```

```{r, eval = FALSE}
# Run duos with 14 cut-points

duos_jagged_350 <- duos(y = y_jagged_350, k = 14)
```

```{r, echo=FALSE}
load("R Objects/duos_jagged_350.RData")
```

`MH_N`: 

This variable represents the number of iterations. For small data sets (i.e. data sets around 100 or less data points), less than 20,000 iterations is most likely enough, but for larger data sets, more iterations sometimes need to be run (See the next section).

```{r, eval = FALSE}
# Run 10,000 iterations on data from N(0, 1) with the default number of cut-points. 
# This data set has 100 points.
duos_norm <- duos(y = y_normal, N = 10000)

# Run 30,000 iterations on data from the 'trimodal' data set
# This data set has 200 data points
duos_trimodal_200 <- duos(y = y_trimodal_200, N = 30000)
```

```{r, echo=FALSE}

load("R Objects/duos_norm.RData")

load("R Objects/duos_trimodal_200.RData")
```


```{r, eval = FALSE}
# Run 10,000 iterations on data from Beta(2,5) with the default number of cut-points
duos_beta <- duos(y = y_beta, N = 10000)

# Run 30,000 iterations on data from Beta(0.5, 0.5) with the default number of cut-points
duos_arcsin_500 <- duos(y = y_arcsin_500, N = 30000)

# For an example of using too few cut-points and iterations
duos_arcsin_500_alt <- duos(y = y_arcsin_500, k = 4, N = 10000)

duos_arcsin <- duos(y = y_arcsin)
```

```{r, echo=FALSE}

load("R Objects/duos_beta.RData")

load("R Objects/duos_arcsin_500.RData")

load("R Objects/duos_arcsin_500_alt.RData")

load("R Objects/duos_arcsin.RData")

```

`alpha`: The prior parameter for the Dirichlet distribution on the bin probabilities. This value is constant, given the bin widths are not knonw, this value is contstant. The recommend and default value is `1`, but the user is allowed to change it.

```{r, eval = FALSE}
# Run with user-specified alpha. 
# The default for a data set with 200 points is 8 cut-points, so alpha
# needs to have 9 values.
duos_trimodal_200_alt <- duos(y = y_trimodal_200, alpha = c(1, 3, 1, 10, 1, 2, 3, 2, 1))
```

```{r, echo=FALSE}
load("R Objects/duos_trimodal_200_alt.RData")
load("R Objects/duos_trimodal_alpha.RData")

```

```{r, eval = FALSE}
# Run with default alpha
duos_trimodal <- duos(y = y_trimodal)

# Run with user-specified alpha
duos_trimodal_alpha <- duos(y = y_trimodal, alpha = c(1, 1, 5, 1, 1, 1))

```

```{r, echo=FALSE}

load("R Objects/duos_trimodal.RData")

load("R Objects/duos_trimodal_alpha.RData")

```

`scale_l` and `scale_u`:

Any data not between 0 and 1, is scaled before the `duos` algorithm is implemented. By default, these values are both 0.0001, however if the user wants to extend the area where the density can be estimted slightly beyond the range of the data, these values allow for this:

$(y-(min(y)-scale_l))/(max(y)+scale_u-(min(y)-scale_l))$

```{r, eval = FALSE}
# Use one standard deviation of the data to scale
duos_norm_scale <- duos(y = y_norm, scale_l = sd(y_norm), scale_u = sd(y_norm))

# For comparison, also run with the default
# Use one standard deviation of the data to scale
duos_norm <- duos(y = y_norm)

```

```{r, echo=FALSE}
load("R Objects/duos_norm_scale.RData")
load("R Objects/duos_norm.RData")
```

`start`: 

Start values are automatically chosen to be `k` random numbers between 0 and 1, and are then sorted. However, the user can specify starting values for the cut-points. This is especially useful for calculating the Gelman-Rubin diagnostic for assessing convergence.

```{r, eval = FALSE}
# Run the algorithm three times. Start the algorithm with values in:
#  (0, 1/3) for run 1, (1/3, 1/3) for run 2, and (2/3, 1) for run 3.  
duos_unif1 <- duos(y = y_unif_50, k = 4, start = c(runif(4, 0,1/3)))
duos_unif2 <- duos(y = y_unif_50, k = 4, start = c(runif(4, 0,1/3)))
duos_unif3 <- duos(y = y_unif_50, k = 4, start = c(runif(4, 0,1/3)))


```



```{r, echo=FALSE}
load("R Objects/duos_unif1.RData")
load("R Objects/duos_unif2.RData")
load("R Objects/duos_unif3.RData")

```

```{r}
# Lode the 'coda' package
library(coda)

# Get the cut-point iterations
C1 <- mcmc(duos_unif1$C)
C2 <- mcmc(duos_unif2$C)
C3 <- mcmc(duos_unif3$C)

# Turn into an MCMC list
C <- mcmc.list(C1, C2, C3)

# Run Gelman-Rubin diagnostic
gelman.diag(C)

# Get the bin probability iterations
P1 <- mcmc(duos_unif1$P)
P2 <- mcmc(duos_unif2$P)
P3 <- mcmc(duos_unif3$P)

# Turn into an MCMC list
P <- mcmc.list(P1, P2, P3)

# Run Gelman-Rubin diagnostic
gelman.diag(P, multivariate = FALSE)

```

#### Convergence Diagnostics

```{r}

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\duos_mcmcplots_unif.jpg", width=1094, height=541)
duos_mcmcplots(duos_unif_50)
dev.off()

```

```{r}

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_Traceplots_Norm.jpg", width=1094, height=541)
duos_mcmcplots(duos_norm)
dev.off()


jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_Traceplots_Norm_Scale.jpg", width=1094, height=541)
duos_mcmcplots(duos_norm_scale)
dev.off()

```


```{r}

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_RM_Trimodal.jpg", width=1094, height=541)
duos_mcmcplots(duos_trimodal_200, type = "rm")
dev.off()


jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_RM_Trimodal_Alt.jpg", width=1094, height=541)
duos_mcmcplots(duos_trimodal_200_alt, type = "rm")
dev.off()

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_ACF_Trimodal.jpg", width=1094, height=541)
duos_mcmcplots(duos_trimodal_200, type = "acf")
dev.off()


jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_acf_Trimodal_Alt.jpg", width=1094, height=541)
duos_mcmcplots(duos_trimodal_200_alt, type = "acf")
dev.off()
```


```{r}


jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_RM_Claw.jpg", width=1094, height=541)
duos_mcmcplots(duos_claw_400, type = "rm")
dev.off()

```

```{r}

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_RM_Normal_P.jpg", width=1094, height=541)
duos_mcmcplots(duos_norm, type = "rm", parameters = "p")
dev.off()


jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_RM_Normal_Alt_P.jpg", width=1094, height=541)
duos_mcmcplots(duos_norm_scale, type = "rm", parameters = "p")
dev.off()

```

```{r}

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_RM_Trimodal_P.jpg", width=1094, height=541)
duos_mcmcplots(duos_trimodal_200, type = "rm", parameters = "p")
dev.off()


jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_RM_Trimodal_Alt_P.jpg", width=1094, height=541)
duos_mcmcplots(duos_trimodal_200_alt, type = "rm", parameters = "p")
dev.off()

```

```{r}


jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_ACF_Claw_P.jpg", width=1094, height=541)
duos_mcmcplots(duos_claw_400, type = "acf", parameters = "p")
dev.off()

```

```{r}
jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_Traceplots_Unif_P.jpg", width=1094, height=541)
# Plot the trace plots of the bin probabilities on separate graphs
#  from the unif(0, 1) data.
duos_mcmcplots(duos_unif_50, parameters = "p", plots = "indiv")
dev.off()
```

```{r}
jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_ACF_Norm_C.jpg", width=1094, height=541)
# Plot the ACF plots of the cut-points on separate graphs
#  from the Norm(0, 1) data with the default scaling.
duos_mcmcplots(duos_norm, type = "acf", plots = "indiv")
dev.off()

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_ACF_Norm_Alt_C.jpg", width=1094, height=541)
# Plot the ACF plots of the cut-points on separate graphs
#  from the Norm(0, 1) data with the default scaling.
duos_mcmcplots(duos_norm_scale, type = "acf",plots = "indiv")
dev.off()
```

```{r}
jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_ACF_Unif_P.jpg", width=1094, height=541)
# Plot the autocorrelation pltos of the bin probabilities overlaid on a single graph
#  from the unif(0, 1).
duos_mcmcplots(duos_unif_50, parameters = "p", type = "acf")
dev.off()
```


```{r}
jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_RM_Trimodal_P.jpg", width=1094, height=541)
duos_mcmcplots(duos_trimodal_200, parameters = "p", type = "rm")
dev.off()


jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_RM_Trimodal_Alt_P.jpg", width=1094, height=541)
duos_mcmcplots(duos_trimodal_200_alt, parameters = "p", type = "rm")
dev.off()
```

```{r}
jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_RM_Claw_C.jpg", width=1094, height=541)

duos_mcmcplots(duos_claw_400, type = "rm", plots = "indiv")
dev.off()

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_RM_Claw_C.jpg", width=1094, height=541)

duos_mcmcplots(duos_claw_400, type = "rm", plots = "indiv")
dev.off()
```

```{r}
#jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_Traceplots_Trimodal_P.jpg", width=1094, height=541)
duos_mcmcplots(duos_trimodal_200, parameters = "p", plots = "indiv")
#dev.off()


#jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_Traceplots_Trimodal_Alt_P.jpg", width=1094, height=541)
duos_mcmcplots(duos_trimodal_200_alt, parameters = "p", plots = "indiv")
#dev.off()
```


```{r}
#jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_Traceplots_Trimodal_C.jpg", width=1094, height=541)
duos_mcmcplots(duos_trimodal_200, plots = "indiv", burnin = 15000)
#dev.off()


#jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_Traceplots_Trimodal_Alt_C.jpg", width=1094, height=541)
duos_mcmcplots(duos_trimodal_200_alt, plots = "indiv", burnin = 10000)
#dev.off()
```



```{r}


jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_Traceplots_Claw_C.jpg", width=1094, height=541)
# Plot the trace plots for the cut-point parameters on individual plots for the results with 13 cut-points. 
duos_mcmcplots(duos_claw_400, plots = "indiv")
dev.off()

```

```{r}
jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_RM_Claw_C.jpg", width=1094, height=541)
# Plot the trace plots for the cut-point parameters on individual plots for the results with 13 cut-points. 
duos_mcmcplots(duos_claw_400, type = "rm", plots = "indiv")
dev.off()

```

```{r}
jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_PP_Unif_C.jpg", width=1094, height=541)
# Posterior verses prior plots on the unif(0, 1) data
duos_pp(duos_unif_50)
dev.off()
```

```{r}

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_PP_Norm_C.jpg", width=1094, height=541)
# Posterior verses prior plots on the N(0, 1) data
# with the default scaling
duos_pp(duos_norm)
dev.off()

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_PP_Norm_Alt_C.jpg", width=1094, height=541)
# Posterior verses prior plots on the unif(0, 1) data
# with alternative scaling
duos_pp(duos_norm_scale)
dev.off()

```


```{r}
jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_PP_Trimodal_P.jpg", width=1094, height=541)
# Posterior verses prior plots on the 'trimodal' data
# with the default 'alpha'
duos_pp(duos_trimodal_200, parameters = "p")
dev.off()

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_PP_Trimodal_Alt_P.jpg", width=1094, height=541)
# Posterior verses prior plots on the unif(0, 1) data
# with alternative 'alpha'
duos_pp(duos_trimodal_200_alt, parameters = "p")
dev.off()
```

```{r}
jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_RM_Claw_C_N.jpg", width=1094, height=541)
# Plot the trace plots for the cut-point parameters on individual plots for the results with 13 cut-points. 
duos_mcmcplots(duos_claw_400_N, type = "rm", plots = "indiv")
dev.off()

```

```{r}
jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_PP_Claw_C.jpg", width=1094, height=541)
# Posterior verses prior plots on the 'trimodal' data
# with the default 'alpha'
duos_pp(duos_claw_400, burnin = 10000)
dev.off()

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_PP_Claw_Alt_C_N.jpg", width=1094, height=541)
# Posterior verses prior plots on the unif(0, 1) data
# with alternative 'alpha'
duos_pp(duos_claw_400_N, burnin = 25000)
dev.off()
```

\subsubsection{duos\_plot}

This function is designed to plot the PDF and CDF based on the output from \textit{duos}. A variety of options are provided to imporve usability and to compare to the original data.


```{r}


jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_PDF_Unif.jpg", width=1094, height=541)
duos_plot(duos_unif_50)
dev.off()

```

```{r}


jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_PDF_Norm.jpg", width=1094, height=541)
duos_plot(duos_norm)
dev.off()

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_PDF_Norm_Alt.jpg", width=1094, height=541)
duos_plot(duos_norm_scale)
dev.off()

```

```{r}


jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_PDF_Trimodal.jpg", width=1094, height=541)
duos_plot(duos_trimodal_200_alt)
dev.off()

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_PDF_Claw.jpg", width=1094, height=541)
duos_plot(duos_claw_400_N)
dev.off()

```

```{r}


c1 <- duos_plot(duos_unif_50, type = "cdf")+ggtitle("Uniform")
c2 <- duos_plot(duos_norm_scale, type = "cdf")+ggtitle("Normal")
c3 <- duos_plot(duos_trimodal_200_alt, type = "cdf")+ggtitle("Trimodal")
c4 <- duos_plot(duos_claw_400_N, type = "cdf")+ggtitle("Claw")

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_CDF_Grid.jpg", width=1094, height=541)
grid.arrange(c1, c2, c3, c4)
dev.off()

```

```{r}

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_PDF_claw_BI.jpg", width=1094, height=541)
duos_plot(duos_claw_400_N, burnin = 30000)
dev.off()
```

```{r}


c1 <- duos_plot(duos_unif_50, cri = TRUE)+ggtitle("Uniform")
c2 <- duos_plot(duos_norm_scale, cri = TRUE)+ggtitle("Normal")
c3 <- duos_plot(duos_trimodal_200_alt, type = "cdf", cri = TRUE)+ggtitle("Trimodal")
c4 <- duos_plot(duos_claw_400_N, type = "cdf", cri = TRUE)+ggtitle("Claw")

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_CRI_Grid.jpg", width=1094, height=541)
grid.arrange(c1, c2, c3, c4)
dev.off()
```



```{r}


c1 <- duos_plot(duos_unif_50, data = TRUE)+ggtitle("Uniform")
c2 <- duos_plot(duos_norm_scale, data = TRUE)+ggtitle("Normal")
c3 <- duos_plot(duos_trimodal_200_alt, data = TRUE)+ggtitle("Trimodal")
c4 <- duos_plot(duos_claw_400_N, data = TRUE)+ggtitle("Claw")

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_Data_PDF_Grid.jpg", width=1094, height=541)
grid.arrange(c1, c2, c3, c4)
dev.off()
```

```{r}


c1 <- duos_plot(duos_unif_50, type = "cdf", data = TRUE)+ggtitle("Uniform")
c2 <- duos_plot(duos_norm_scale, type = "cdf", data = TRUE)+ggtitle("Normal")
c3 <- duos_plot(duos_trimodal_200_alt, type = "cdf", data = TRUE)+ggtitle("Trimodal")
c4 <- duos_plot(duos_claw_400_N, type = "cdf", data = TRUE)+ggtitle("Claw")

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_Data_CDF_Grid.jpg", width=1094, height=541)
grid.arrange(c1, c2, c3, c4)
dev.off()
```

```{r}


duos_plot(duos_norm_scale, type = "cdf", data = TRUE)+ggtitle("Normal")

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_Data_PDF_Data_CRI.jpg", width=1094, height=541)
duos_plot(duos_norm_scale, cri = TRUE, data = TRUE)+ggtitle("Normal")
dev.off()
```

```{r}
duos_plot(duos_trimodal_200_alt, type = "cdf", interact = TRUE)
```


```{r}
duos_plot(duos_unif_50, cri = TRUE, interact = TRUE)
```


```{r}
duos_plot(duos_norm_scale, data = TRUE, cri = TRUE, interact = TRUE)
```

```{r}
duos_plot(duos_claw_400_N, type = "cdf", data = TRUE, cri = TRUE, interact = TRUE)
```


#### duos_pdf

```{r}
duos_pdf(x = c(.2, .4, 1.2),duos_output = duos_unif_50)
```



```{r}

duos_pdf(x = c(0, 1.5, -2.9,-3.1, 4),duos_output = duos_claw_400_N)


```

```{r}

# Estimated the density at (0.2, 0.4, 0.6, 0.8) for
# the unif(0, 1) data
duos_pdf_unif <- duos_pdf(x = c(0.2, 0.4, 0.6, 0.8), duos_output = duos_unif_50)

duos_pdf_unif$pdf

duos_pdf_unif$cri

```

```{r}
data.frame(duos_pdf_unif$mat) %>% gather(Variable, Value) %>%
  ggplot(aes(Value, fill = Variable, color = Variable))+
  geom_histogram(position = "identity", alpha = 0.5, bins = 40)+
  theme_bw()+theme(axis.title =  element_text(size = 12))

jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_PDF_MAT_Unif.jpg", width=1094, height=541)
data.frame(duos_pdf_unif$mat) %>% gather(Variable, Value) %>%
  ggplot(aes(Value, fill = Variable, color = Variable))+
  geom_histogram(alpha = 0.5, bins = 40)+
  theme_bw()+theme(axis.title =  element_text(size = 12))+
  facet_wrap(~Variable, nrow = 4)+xlim(c(0, 3))
dev.off()
  
```

```{r}
# User iteractive identify locations of possible modes
duos_plot(duos_norm_scale, interact = TRUE)
```


```{r}
# User iteractive identify range of values to check for mode
duos_plot(duos_norm_scale, interact = TRUE)

# Estimated the density at seq(from = -.6, to = .65, by = 0.001) for
# the 'claw' data
x_seq <- seq(from = -.6, to = .65, by = 0.001)
duos_pdf_norm <- duos_pdf(x = x_seq, duos_output = duos_norm_scale)

find_max <- which(duos_pdf_norm$pdf==max(duos_pdf_norm$pdf))
x_seq[find_max]

duos_pdf_norm$cri[find_max,]

```

```{r}
# Estimated the density at (-1, -0.5, .025, 0.54, 1) for
# the 'claw' data
duos_pdf_claw <- duos_pdf(x = c(-2.5, -2, 2.5, 3), duos_output = duos_claw_400_N)

duos_pdf_claw$pdf

duos_pdf_claw$cri

```

#### cdf

```{r}
duos_cdf_unif <- duos_cdf(x = c(0.25, 0.5, 0.75), duos_output = duos_unif_50)

duos_cdf_unif$cdf

duos_cdf_unif$cri

```



```{r}
duos_cdf_norm <- duos_cdf(x = c(min(duos_norm_scale$y), max(duos_norm_scale$y)), duos_output = duos_norm_scale)

# Find the probability of being less than the minimum value
duos_cdf_norm$cdf[1]

# Find the probability of being greater than the maximum value
1-duos_cdf_norm$cdf[2]

duos_cdf_norm$cri

```

```{r}
duos_plot(duos_trimodal_200_alt, interact = TRUE)

duos_cdf_trimodal <- duos_cdf(x = c(1.76, 2.3), duos_output = duos_trimodal_200_alt)

# Find the probability of being between 1.76 and 2.3
duos_cdf_trimodal$cdf[2]-duos_cdf_trimodal$cdf[1]

# Find the credible interval using the matrix
# of values
quantile(duos_cdf_trimodal$mat[,2]-duos_cdf_trimodal$mat[,1], c(.025, .975))

```

```{r}
duos_plot(duos_claw_400_N, interact = TRUE)

duos_cdf_claw <- duos_cdf(x = c(-1.26, -0.7, -0.2, 0.25, 0.8, 1.3), duos_output = duos_claw_400_N)

# Find the probability of being between each pair of points
duos_cdf_claw$cdf[2]-duos_cdf_claw$cdf[1]
duos_cdf_claw$cdf[3]-duos_cdf_claw$cdf[2]
duos_cdf_claw$cdf[4]-duos_cdf_claw$cdf[3]
duos_cdf_claw$cdf[5]-duos_cdf_claw$cdf[4]
duos_cdf_claw$cdf[6]-duos_cdf_claw$cdf[5]

# Find the credible interval using the matrix
# of values
quantile(duos_cdf_trimodal$mat[,2]-duos_cdf_trimodal$mat[,1], c(.025, .975))

cdf_mat <- data.frame(Mode1 = duos_cdf_claw$mat[,2]-duos_cdf_claw$mat[,1],
                      Mode2 = duos_cdf_claw$mat[,3]-duos_cdf_claw$mat[,2],
                      Mode3 = duos_cdf_claw$mat[,4]-duos_cdf_claw$mat[,3],
                      Mode4 = duos_cdf_claw$mat[,5]-duos_cdf_claw$mat[,4],
                      Mode5 = duos_cdf_claw$mat[,6]-duos_cdf_claw$mat[,5])

# Plot the histograms
jpeg(file = "D:\\Documents\\Dissertation - Summer 2018\\Latex Dissertation\\Images\\V_CDF_MAT_Claw.jpg", width=1094, height=541)
  cdf_mat %>% gather(Variable, Value) %>%
    ggplot(aes(Value, fill = Variable, color = Variable))+
  geom_histogram(position = "identity", alpha = 0.5, bins = 40)+
  theme_bw()+theme(axis.title =  element_text(size = 12))
dev.off()




```
#### Additional statistics

```{r}

duos_unif_mean <- duos_stat(duos_unif_50)
```

### Gold

#### Density Estimation

#### Convergence Diagnostics

#### CDF and PDF plots

#### Additional statistics

Vignettes are long form documentation commonly included in packages. Because they are part of the distribution of the package, they need to be as compact as possible. The `html_vignette` output type provides a custom style sheet (and tweaks some options) to ensure that the resulting html is as small as possible. The `html_vignette` format:

- Never uses retina figures
- Has a smaller default figure size
- Uses a custom CSS stylesheet instead of the default Twitter Bootstrap style

## Vignette Info

Note the various macros within the `vignette` section of the metadata block above. These are required in order to instruct R how to build the vignette. Note that you should change the `title` field and the `\VignetteIndexEntry` to match the title of your vignette.

## Styles

The `html_vignette` template includes a basic CSS theme. To override this theme you can specify your own CSS in the document metadata as follows:

    output: 
      rmarkdown::html_vignette:
        css: mystyles.css

## Figures

The figure sizes have been customised so that you can easily put two images side-by-side. 

```{r, fig.show='hold'}
plot(1:10)
plot(10:1)
```

You can enable figure captions by `fig_caption: yes` in YAML:

    output:
      rmarkdown::html_vignette:
        fig_caption: yes

Then you can use the chunk option `fig.cap = "Your figure caption."` in **knitr**.

## More Examples

You can write math expressions, e.g. $Y = X\beta + \epsilon$, footnotes^[A footnote here.], and tables, e.g. using `knitr::kable()`.

```{r, echo=FALSE, results='asis'}
knitr::kable(head(mtcars, 10))
```

Also a quote using `>`:

> "He who gives up [code] safety for [code] speed deserves neither."
([via](https://twitter.com/hadleywickham/status/504368538874703872))
