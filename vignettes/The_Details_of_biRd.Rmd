---
title: "The Details of biRd"
author: "Kathleen P. Rey"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    fig_width: 8
    fig_height: 4
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(biRd)
```

## Introduction

Bayesian!

Nonparametric!!!!

Ease of implementation!!!

## Methodology

### DUOS

#### Introduction

The Bayesian density estimator in this section is designed to have little to know input necessary from the user. Although, knowledge of the complexity of the density and the size of the data can definitley and should influence choices in the input, this method can make reasonable choices without user input other than the data. This method is a histogram-like density estimate, but rather than the widths being fixed or the widths varying, but the midpoints being fixed, the end points of the bins are random. The number of bins is chosen by the user, or as seen later, can be left to the default.

The positions of the bin end points (or cut-points as they are refered to throughout the vignette) are random order statistics on unif(0,1). From here on, this method will be refered to as DUOS: Distribution of Uniform Order Statistics.

Before going into detail, an example of what `duos` is creating is below. The resultin gposterior mean density (in red) is a mean of many different step functions (in blue). These are both0 overalayed over a histogram of the actual data. 
 
```{r, echo = FALSE}

# Read in sample data
y <- read.csv("Data/Data_Vshape_Size300_Rep4.csv")
y <- y$x

# Cut- points from a set of test data
C <- read.csv("Data/C_Vshape_300_c7_Rep4.csv", header=TRUE)
# Bin proporitions from the same set of test data
P <- read.csv("Data/P_Vshape_300_c7_Rep4.csv", header=TRUE)

# Remove the first column with is row numbers
C <- C[,-1]
P <- P[,-1]

# Set the burnin so not plotting too much output
Burnin <- 19800
# Set the number of cut-points to a global parameters
k <<- ncol(C)

# Create a grid to estimate the densities on
input <<- seq(0.01,.99, by=.01)

# Pdf function
pdf <- function(x){
    pi <- rep(0,length(input))
    c_full <- c(0, x[1:k],1)
    p <- x[{k+1}:length(x)]
    for (j in 1:(k+1)){
      pi[which(input<c_full[(j+1)] & input>=c_full[j])]<- p[j]/(c_full[(j+1)]-c_full[j])
    }
    return(pi)
  }

# Get subsets of iterations
C_sub <- C[Burnin:nrow(C),]
P_sub <- P[Burnin:nrow(C),]

# Estimate the density at each iteration 
pdf_y_matrix <- apply(cbind(C_sub,P_sub), 1, pdf)

# Data to plot
pdf_plot <- data.frame(X=input, pdf_y_matrix)

# Stack data
pdf_plot <- pdf_plot %>% gather(Run, Density, -X) %>% filter(Density<5)

# Data to use as histogram
j_hist <- data.frame(y=y)

# Calculate the mean density estimate
pdf_mean <- data.frame(X=input, PDF= apply(pdf_y_matrix, 1, mean))

ggplot(data=j_hist, aes(y))+
  geom_line(data=pdf_plot, aes(X, Density,group=Run, color="blue"),alpha=.9)+
    geom_histogram(aes(y=..density..),fill="grey", color="black",alpha=.9)+
  geom_line(data=pdf_mean, aes(X, PDF, color="red"), size=1)+
  theme_bw()+ylab("Posterior Mean Density")+xlab("X")+
  theme(text = element_text(size=15),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  scale_colour_manual(name = '',
  values =c('blue'='blue','red'='red'), labels = c('Single Iteration','Posterior Mean'))

```

The particular results from above were simulated from a distribution that lies between 0 and 1. `duos` is designed to work on data that is in the range (0,1). If the data is not between 0 and 1, is is standardize using the options in section [Density Estimation](#duos-density-estimation). As mentioned above, the data model is a step function as specified below:

* Let $k$ = the number of cut-points.
* Let $\gamma_j$ represent a cut-point: j = 1,..k.
* Let $\pi_j$ represent the proportion associated with the bin ($\gamma_{j-1}, \gamma_j$)

$f_{\boldsymbol\gamma,\boldsymbol\pi}(x)$ = \left\
        \begin{array}{ll}
            \frac{\pi_1}{(\gamma_1)} & \quad 0 \leq x  < \gamma_1 \\
            \frac{\pi_2}{(\gamma_2-\gamma_1)} & \quad \gamma_1 \leq x  < \gamma_2 \\
            \frac{\pi_3}{(\gamma_3-\gamma_2)} & \quad \gamma_2 \leq x  < \gamma_3 \\
            ... & \quad ... \leq x  < ... \\
            \frac{\pi_k}{(\gamma_k-\gamma_{k-1})} & \quad \gamma_{k-1} \leq x  < \gamma_k \\
            \frac{\pi_{k+1}}{(1-\gamma_k)} & \quad \gamma_k \leq x  < 1 \\
        \end{array}

$k$ : the number of cut points \newline
\indent
$\boldsymbol\gamma : 0 < \gamma_1 < \gamma_2 < ... < \gamma_{k-1} < \gamma_k < 1$ \newline
\indent
$\boldsymbol\pi : \pi_k \geq 0$ and  $\sum_{j=1}^{k+1} \pi_j = 1$

$\boldsymbol\gamma$ and $\boldsymbol\pi$ are the unknown parameters and therefore, require priors.

Assume $boldsymbol\gamma$ and  $boldsymbol\pi$ are independent (i.e. $p(\boldsymbol\gamma, \boldsymbol\pi) = p(\boldsymbol\gamma)p(\boldsymbol\pi)$).

**Priors:**

$\boldsymbol\gamma \sim$ *DUOS prior*

i.e. Draw k values from $unif(0,1)$ and order: $0 < \gamma_1 < \gamma_2 < ... < \gamma_{k-1} < \gamma_k < 1$.

$\boldsymbol\pi \sim Dir(\boldsymbol\alpha)$

where $\alpha_j = 1$ for $j = 1,2,...,k,k+1$ 

Noninformative priors were chosen so that little informational input is required from the user.

Let $x_1, x_2,...,x_n$ be independent and identical samples from some unknown distribution.

`duos` implements a Gibbs algorithm using the following full conditionals:

$p(\boldsymbol\pi|\textbf x,\boldsymbol\gamma) \sim Dir(\alpha^*)$

where $\alpha^* =(1+\sum_{i=1}^{n} \mathrm{I}(0\leq x_i<\gamma_1), 1+\sum_{i=1}^{n} \mathrm{I}(\gamma_1\leq x_i<\gamma_2),..., 1+\sum_{i=1}^{n}\mathrm{I}(\gamma_k\leq x_i<1))$

Note the meaning of the notation $\boldsymbol\gamma_{-j}$ used below. This indicates the $\gamma_j$ is conditioning on all other $\gamma_j$'s except $\gamma_j$. Thus, the full conditional for each $\gamma_j$ is as follows:

$p(\gamma_j|\textbf x,\boldsymbol\pi,\boldsymbol\gamma_{-j}) \propto {\frac{\pi_j}{\gamma_j-\gamma_{j-1}}}^{\sum_{i=1}^{n}\mathrm{I}(\gamma_{j-1}\leq x_i<\gamma_j)}\frac{\pi_{j+1}}{\gamma_{j+1}-\gamma_j}^{\sum_{i=1}^{n}\mathrm{I}(\gamma_j \leq x_i<\gamma_{j+1})}$
where $\gamma_j \in [\gamma_{j-1}, \gamma_{j+1})$

Although the full conditional for the bin probability parameters is a known distribution, the full conditional for the cut-point parameters is not.  

```{r, echo = FALSE}
# Code to creatae plot of what a full conditional distribution for a cut-point might look like
# w is cut-point, row is with row want to create plot from, and nsim is how many values to calculate kernel at 

w <- 3
row <- 15000
nsim <- 1000

# Read in data sample
y <- read.csv("Data/Data_Vshape_Size300_Rep4.csv")
ys <- y$x

# Read in cut-points and bin probabilities
C <- read.csv("Data/C_Vshape_300_c7_Rep4.csv", header=TRUE)
P <- read.csv("Data/P_Vshape_300_c7_Rep4.csv", header=TRUE)
# Remove first column that contains row numberes
C <- C[,-1]
P <- P[,-1]

# Get cut-points
c <- as.numeric(C[row,])
# Add in - and 1
Q_C_extra <- c(0, c, 1)
# index parameter
w_p2 <- w+2
# Get data between bordering cut-points
xm <- c(Q_C_extra[w], ys[ys>=Q_C_extra[w]&ys<Q_C_extra[{w+2}]], Q_C_extra[{w+2}])
# find number of observatiosn between C[w-1] and C[w+1]
m <- length(xm)-2

# Initialize vector to contains maximums on each interval
c_max <- NA
# Initialize vector to contain minimums on each interval
# Get for first interval in the vector
c_max_choices <- c(xm[1], xm[2])
# The maximum from derivatives is Q_C_extra[w] which is xm[1]
# Pairs (c(1/2), c(1/3), c(2/3))
# Create to contain ratios of the pdf's

# option1/option2
c_max_maxes <- (Q_C_extra[w_p2]-c_max_choices[1])^(-m)/((Q_C_extra[w_p2]-c_max_choices[2])^(-m))

# Compare ratio to 1 to find maximum
if (c_max_maxes>1){
  c_max[1] <- c_max_choices[1]
} else{
  c_max[1] <- c_max_choices[2]
}

# If more than one observation between cut-points
if (m >1){

  for (q in 2:m){


    q_p1 <- q+1
    # Counter for data as upper bound, negative since power for denominator

    m1_p1 <- -q+1
    mm_pq_m1 <- -m+q-1
    # Choices for maximum
    c_max_choices <- c(xm[q], xm[q_p1], (((m-q+1)*Q_C_extra[w]+(q-1)*Q_C_extra[w_p2])/m))

    # Pairs (c(1/2), c(1/3), c(2/3))
    c_max_maxes <- NA
    # option1/option2
    c_max_maxes[1] <- exp(m1_p1*log(c_max_choices[1]-Q_C_extra[w])+mm_pq_m1*log(Q_C_extra[w_p2]-c_max_choices[1])-m1_p1*log(c_max_choices[2]-Q_C_extra[w])-mm_pq_m1*log(Q_C_extra[w_p2]-c_max_choices[2]))
    # Option1/option3
    c_max_maxes[2] <- exp(m1_p1*log(c_max_choices[1]-Q_C_extra[w])+mm_pq_m1*log(Q_C_extra[w_p2]-c_max_choices[1])-m1_p1*log(c_max_choices[3]-Q_C_extra[w])-mm_pq_m1*log(Q_C_extra[w_p2]-c_max_choices[3]))
    # Option2/option3
    c_max_maxes[3] <- exp(m1_p1*log(c_max_choices[2]-Q_C_extra[w])+mm_pq_m1*log(Q_C_extra[w_p2]-c_max_choices[2])-m1_p1*log(c_max_choices[3]-Q_C_extra[w])-mm_pq_m1*log(Q_C_extra[w_p2]-c_max_choices[3]))

    # Compare ratio to 1 to find maximum
    if (c_max_choices[3] >= xm[q] & c_max_choices[3] < xm[q_p1]){

      if (c_max_maxes[1] > 1 & c_max_maxes[2] > 1) {

        c_max[q] <- c_max_choices[1]
      } else if (c_max_maxes[1] < 1 & c_max_maxes[3] > 1) {

        c_max[q] <- c_max_choices[2]
      } else {
        c_max[q] <- c_max_choices[3]
      }
    } else if ( c_max_maxes[1] > 1) {

      c_max[q] <- c_max_choices[1]
    } else {
      c_max[q] <- c_max_choices[2]
    }


  }
}

# GET for last
c_max_choices <- c(xm[m+1], xm[m+2])

#  Pairs (c(1/2), c(1/3), c(2/3))
#option1/option2
c_max_maxes <- (c_max_choices[1]-Q_C_extra[w])^(-m)/((c_max_choices[2]-Q_C_extra[w])^(-m))

if (c_max_maxes[1] > 1){
  c_max[m+1] <- c_max_choices[1]
} else {
  c_max[m+1] <- c_max_choices[2]
}

# Get bin probabilities
p <- P[row,]
# Values at which to calculate kernel
x <- runif(nsim, c[{w-1}], c[{w+1}])

# Calculate the kernel of the density
pdf_kernel <- NA
for (i in 1:length(x)){
  n1 <- length(which(ys>=c[{w-1}]&ys<x[i]))
  n2 <- length(which(ys<c[{w+1}]&ys>=x[i]))

  pdf_kernel[i] <- (p[w]/(x[i]-c[{w-1}]))^n1*(p[{w+1}]/(c[{w+1}]-x[i]))^n2
   #print(i)
}

# Calculate what bounding function would look like
bound_plot <- NA
for (i in 1:{length(xm)-1}){
  n1 <- i-1
  n2 <- length(xm)-2-i+1

  bound_plot[which(x>=xm[i]&x<xm[{i+1}])] <- (p[w]/(c_max[i]-c[{w-1}]))^n1*(p[{w+1}]/(c[{w+1}]-c_max[i]))^n2
}

# Make and sort data frame
d2 <- data.frame(x,pdf_kernel, bound_plot)
d2 <- d2 %>% arrange(x)

# Create data set to identify discontinuities in graph
locations <- NA
index <- 1
for (i in 1:{length(xm)-1}){
  if (length(which(d2$x>=xm[{i+1}])>0)){
    locations[index] <- min(which(d2$x>=xm[{i+1}]))
    index <- index+1
  }
}

d2$disc <- "Cont"
d2$disc[locations] <- "Disc"

options(digits=4)

ggplot(d2)+geom_point(aes(x,pdf_kernel, shape=disc, size=disc))+
  theme_bw()+xlab("x")+ylab("Full Conditional Kernel")+
  theme(text = element_text(size=15),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        legend.position = "none")+
  scale_shape_manual(values=c(16,1))+
  scale_size_manual(values=c(1,4))

```

Not the open circles where discontinuities exist. The breaks in the kernel of the full conditional for a cut-point occur at the data points. This is because, between two data points from the sample, the counts in the powers in the full conditional are constant. 

Given the complecated nature of this density, it is numerically difficult to find the normalizing constant, not to mention simulating from this distribution. Thus, in order to obtain samples from this distribution, rejection sampling was implemented. The proposal (and boudning) distribution used is depicted in red in the graph below.

```{r, echo = FALSE}
ggplot(d2)+geom_point(aes(x,pdf_kernel, shape=disc, size=disc))+
  geom_point(aes(x, bound_plot), color="red",size=1.5)+
  theme_bw()+xlab("x")+ylab("Kernel")+
  theme(text = element_text(size=20),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        legend.position = "none")+
  scale_shape_manual(values=c(16,1))+
  scale_size_manual(values=c(1,4))

```

### GOLD

## R Package: biRD (Bayesian Implimention in R for Density estimation)

Demonstrations of the functions for `duos` and `gold` are demonstrated on a wide variety of distributions. These are depicted in the plot below.


```{r, echo = FALSE}
# Grid to estimate densities on
x <- seq(from=.001, to=.999, by=.001)
# Unif
Uniform <- rep(1, length(x))

# Exponential
Exponential <- dexp(x, 1)

# Arcsin
Arcsin <- dbeta(x, .5,.5)

# Beta(2,5)
Beta <- dbeta(x, 2,5)

# Jagged
Jagged <- NA
Jagged[x>=0 & x <.2] <- 1.5-5*x[x>=0 & x<.2]
Jagged[x>=.2 & x <.4] <- -0.5+5*x[x>=.2 & x<.4]
Jagged[x>=.4 & x <.6] <- 3.5-5*x[x>=.4 & x<.6]
Jagged[x>=.6 & x <.8] <- -2.5+5*x[x>=.6 & x<.8]
Jagged[x>=.8 & x <1] <- 5.5-5*x[x>=.8 & x<1]

# Normal
y <- x*(3.999+3.999)+-3.999
Normal <- dnorm(y)

# Bimodal
# Sample from uniform
u <- runif(100000)

# Variable to store the samples from the mixture distribution
rs <- rep(NA,100000)

#Sampling from the mixture
for(i in 1:100000){
  if(u[i]<.3){
    rs[i] <- rnorm(1,0,1)
  }else {
    rs[i] <- rnorm(1,4,1)
  }
}
# Rescale
y <- x*(max(rs)-min(rs))+min(rs)
Bimodal <- .3*dnorm(y,0,1) + .7*dnorm(y,4,1)

# Claw
# Sample from uniform
u <- runif(100000)

# Variable to store the samples from the mixture distribution
rs <- rep(NA,100000)

#Sampling from the mixture
for(i in 1:100000){
  if(u[i]<.5){
    rs[i] <- rnorm(1,0,1)
  }else if(u[i]<.6){
    rs[i] <- rnorm(1,-1,.1)
  }else if (u[i] < .7){
    rs[i] <- rnorm(1,-.5,.1)
  }else if (u[i]<.8){
    rs[i] <- rnorm(1,0,.1)
  }else if (u[i] < .9){
    rs[i] <- rnorm(1,.5,.1)
  }else {
    rs[i] <- rnorm(1,1,.1)
  }
}

# Rescale
y <- x*(max(rs)-min(rs))+min(rs)
Claw <- .5*dnorm(y)+.1*dnorm(y,-1,.1)+.1*dnorm(y,-.5,.1)+.1*dnorm(y,0,.1)+.1*dnorm(y,.5,.1)+.1*dnorm(y,1,.1)

# Trimodal
# Sample from uniform
u <- runif(100000)
# Variable to store the samples from the mixture distribution
rs <- rep(NA,100000)

# Sampling from the mixture
for(i in 1:100000){
  if(u[i]<.2){
    rs[i] <- rnorm(1,0,1)
  }else if(u[i]<.5){
    rs[i] <- rnorm(1,6,1)
  }else{
    rs[i] <- rnorm(1,2,.1)
  }
}

y <- x*(max(rs)-min(rs))+min(rs)
Trimodal = .2*dnorm(y,0,1) + .5*dnorm(y,6,1) + .3*dnorm(y,2,.1)

distributions <- data.frame(x, Uniform, Arcsin, Beta, Exponential, Jagged, Normal, Bimodal,Trimodal, Claw)

d_melt <- gather(distributions, variable, value, -x)
d_melt$variable <- factor(d_melt$variable, levels=c("Uniform", "Beta","Normal","Exponential", "Arcsin",
                                                    "Bimodal","Jagged", "Trimodal","Claw"))

ggplot(d_melt, aes(x, value))+geom_line(size=1,color="dodgerblue4")+
  facet_wrap(~variable, nrow=3, scales="free_y")+
  theme_bw()+
  theme(text = element_text(size=25),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        legend.position = "none")
  #scale_color_viridis(discrete=TRUE,option="magma",end=.8)
  #scale_color_brewer(palette = "Set1")

```

### DUOS

The package is desinged to:
* Run Gibbs algorithm to sample from the posterior distributions for the cut-points and bin probabilities
* Provide useful convergence diagnostic plots
* Plot the Bayesian estimate of the PDF and CDF
* Give estimates of the PDF and CDF at new values
* Estimate a variety of statistics 

All of the functionally of the package is demonstrated by walking through the analysis of several sets of simulated data.

#### Density Estimation {#duos-density-estimation}

The code below creates data from each of the nine densitites described above. For the purposes of demonstration, a sample set of size 100 is used for proceed with most results. However, plots will be used to demonstrate overall how the estimate changes as the sample size increases. The plot below shows the histograms of the sampled data.

```{r, echo = FALSE}
# # Sample size
# n <- 100
# 
# # Uniform
# y_unif <-  runif(n, 0, 1)
# y_unif_500 <- runif(500, 0, 1)
# 
# # Beta
# y_beta <- rbeta(n, 2, 5)
# 
# # Normal
# y_norm <- rnorm(n, 0, 1)
# 
# # Exponential
# y_exp <- rexp(n, 1)
# 
# # Arcsin
# y_arcsin <- rbeta(n, 0.5, 0.5)
# 
# y_arcsin_500 <- rbeta(500, 0.5, 0.5)
# # Bimodal
#   # Sample from uniform
#   u <- runif(n)
#   
#   # Variable to store data                                        
#   y_bimodal <- rep(NA,n)
#   
#   # Sampling from the mixture
#   for(i in 1:n){
#       if(u[i]<.3){
#         y_bimodal[i] <- rnorm(1, 0, 1)
#       }else {
#         y_bimodal[i] <- rnorm(1, 4, 1)
#       }
#   }
# 
# # Jagged
#   y1 = 1.5
#   y2 = .5
#     
#   m1 = (y2-y1)/.2
#   m2 = (y1-y2)/.2
#   m3 = (y2-y1)/.2
#   m4 = (y1-y2)/.2
#   m5 = (y2-y1)/.2
#   
#   b1 = y1
#   b2 = 2*y2-y1
#   b3 = 3*y1-2*y2
#   b4 = 4*y2-3*y1
#   b5 = 5*y1-4*y2
#     
#   x_jag = runif(n, 0, 1)
#     
#   y_jagged <- NA
#   y_jagged[x_jag>=0 & x_jag<.2] <- (-1.5+sqrt(1.5^2-10*x_jag[x_jag>=0 & x_jag<.2]))/(-5)
#   y_jagged[x_jag>=.2 & x_jag<.4] <- (.5+sqrt(.5^2-4*(5/2)*(.2-x_jag[x_jag>=.2 & x_jag<.4])))/(5)
#   y_jagged[x_jag>=.4 & x_jag<.6] <- (-3.5+sqrt(3.5^2-4*(-5/2)*(-.6-x_jag[x_jag>=.4 & x_jag<.6])))/(-5)
#   y_jagged[x_jag>=.6 & x_jag<.8] <- (2.5+sqrt(2.5^2-4*(5/2)*(1.2-x_jag[x_jag>=.6 & x_jag<.8])))/(5)
#   y_jagged[x_jag>=.8 & x_jag<1] <- (-5.5+sqrt(5.5^2-4*(-5/2)*(-2-x_jag[x_jag>=.8 & x_jag<1])))/(-5)
#   
#   
#   # Trimodal
#     u <- runif(n)
#                                          
#     y_trimodal <- rep(NA,n)
#     
#     #Sampling from the mixture
#     for(i in 1:n){
#       if(u[i]<.2){
#         y_trimodal[i] <- rnorm(1, 0, 1)
#       }else if(u[i]<.5){
#         y_trimodal[i] <- rnorm(1, 6, 1)
#       }else{
#         y_trimodal[i] <- rnorm(1, 2, 0.1)
#       }
#     }
#   
# # Claw
#     u <- runif( n)
#     
#     y_claw = rep(NA, n)
#     
#     # Sampling from the mixture
#     for(i in 1: n){
#       if(u[i]<.5){
#         y_claw[i] <- rnorm(1, 0, 1)
#       }else if(u[i]<.6){
#         y_claw[i] <- rnorm(1, -1, 0.1)
#       }else if (u[i] < .7){
#         y_claw[i] <- rnorm(1, -0.5, 0.1)
#       }else if (u[i]<.8){
#         y_claw[i] <- rnorm(1, 0, 0.1)
#       }else if (u[i] < .9){
#         y_claw[i] <- rnorm(1, 0.5, 0.1)
#       }else {
#         y_claw[i] <- rnorm(1, 1, 0.1)
#       }
#     }
# 
#     
# distributions <- data.frame(Uniform = y_unif, Beta = y_beta, Normal= y_norm, Exponential = y_exp, Arcsin = y_arcsin, Bimodal = y_bimodal, Jagged = y_jagged, Trimodal = y_trimodal,Claw = y_claw)
# 
# distr_gather <- gather(distributions, variable, value)
# distr_gather$variable <- factor(distr_gather$variable, levels=c("Uniform", "Beta","Normal","Exponential",
#                                                                 "Arcsin","Bimodal","Jagged", "Trimodal","Claw"))

distr_gather <- read.csv("Data/distr_gather.csv")

 distr_gather$variable <- factor(distr_gather$variable, levels=c("Uniform", "Beta","Normal","Exponential", "Arcsin","Bimodal","Jagged","Trimodal","Claw"))
ggplot(distr_gather, aes(value))+geom_histogram(color="black", fill = "grey", bins = 15)+
  facet_wrap(~variable, nrow=3, scales="free")+
  theme_bw()+
  theme(text = element_text(size=20),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        legend.position = "none")

y_unif <- distr_gather$value[distr_gather$variable=="Uniform"]
y_beta <- distr_gather$value[distr_gather$variable=="Beta"]
y_normal <- distr_gather$value[distr_gather$variable=="Normal"]
y_Exp <- distr_gather$value[distr_gather$variable=="Exponential"]
y_arcsin <- distr_gather$value[distr_gather$variable=="Arcsin"]
y_bimodal <- distr_gather$value[distr_gather$variable=="Bimodal"]
y_jagged <- distr_gather$value[distr_gather$variable=="Jagged"]
y_trimodal <- distr_gather$value[distr_gather$variable=="Trimodal"]
y_claw <- distr_gather$value[distr_gather$variable=="Claw"]

y_unif_500 <- read.csv("Data/y_unif_500.csv")[,1]
y_arcsin_500 <- read.csv("Data/y_arcsin_500.csv")[,1]


```

The function to start with is `duos`. This is the function that runs the actual Gibbs algorithm. Prior values are set in this function as well as the number of iterations. 

`k`: 

This is the number of cut-points to have `duos` use. If k cut-points are chosen, there are k+1 bins. \textbf{More cut-points} are needs as the sample size increases. Typically, this is an additional cut-point for each additional 50 data points. It is recommended that you start with at least $k = 3$. This default is incorporated in to the function. Note that a counter is printed every 1000 iterations to notify you on the process of `duos`.

```{r, eval=FALSE}
# If you run 'duos' with all defaults, it automatically chooses the number of cutpoints based on the sample size

duos_unif <- duos(y = y_unif)

```

```{r, echo=FALSE}
load("R Objects/duos_unif.RData")
```

```{r}
duos_unif$k

```

However, for some densities, more cut-points might be needed based on prior knowledge of the data.

```{r, eval = FALSE}
# If you run 'duos' with all defaults, it automatically chooses the number of cutpoints based on the sample size

duos_claw <- duos(y = y_claw, k = 10)
```

```{r, echo=FALSE}
load("R Objects/duos_claw.RData")
```

`MH_N`: 

This variable represents the number of iterations. For small data sets (i.e. data sets around 100 or less data points), less than 20,000 iterations is most likely enough, but for larger data sets, more iterations sometimes need to be run (See the next section).

```{r, eval = FALSE}
# Run 10,000 iterations on data from Beta(2,5) with the default number of cut-points
duos_beta <- duos(y = y_beta, MH_N = 10000)

# Run 30,000 iterations on data from Beta(0.5, 0.5) with the default number of cut-points
duos_arcsin_500 <- duos(y = y_arcsin_500, MH_N = 30000)

```

```{r, echo=FALSE}

load("R Objects/duos_beta.RData")

load("R Objects/duos_arcsin_500.RData")

```

`alpha`: The prior parameter for the Dirichlet distribution on the bin probabilities. This value is constant, given the bin widths are not knonw, this value is contstant. The recommend and default value is `1`, but the user is allowed to change it.


`scale_l` and `scale_u`:

Any data not between 0 and 1, is scaled before the `duos` algorithm is implemented. By default, these values are both 0.0001, however if the user wants to extend the area where the density can be estimted slightly beyond the range of the data, these values allow for this:

$(y-(min(y)-scale_l))/(max(y)+scale_u-(min(y)-scale_l))$

```{r, eval = FALSE}
# Use one standard deviation of the data to scale
duos_norm_scale <- duos(y = y_norm, scale_l = sd(y_norm), scale_u = sd(y_norm))

# For comparison, also run with the default
# Use one standard deviation of the data to scale
duos_norm <- duos(y = y_norm)

```

```{r, echo=FALSE}
load("R Objects/duos_norm_scale.RData")
load("R Objects/duos_norm.RData")
```

`start`: 

Start values are automatically chosen to be `k` random numbers between 0 and 1, and are then sorted. However, the user can specify starting values for the cut-points. This is especially useful for calculating the Gelman-Rubin diagnostic for assessing convergence.



#### Convergence Diagnostics

#### CDF and PDF plots

#### Additional statistics

### Gold

#### Density Estimation

#### Convergence Diagnostics

#### CDF and PDF plots

#### Additional statistics

Vignettes are long form documentation commonly included in packages. Because they are part of the distribution of the package, they need to be as compact as possible. The `html_vignette` output type provides a custom style sheet (and tweaks some options) to ensure that the resulting html is as small as possible. The `html_vignette` format:

- Never uses retina figures
- Has a smaller default figure size
- Uses a custom CSS stylesheet instead of the default Twitter Bootstrap style

## Vignette Info

Note the various macros within the `vignette` section of the metadata block above. These are required in order to instruct R how to build the vignette. Note that you should change the `title` field and the `\VignetteIndexEntry` to match the title of your vignette.

## Styles

The `html_vignette` template includes a basic CSS theme. To override this theme you can specify your own CSS in the document metadata as follows:

    output: 
      rmarkdown::html_vignette:
        css: mystyles.css

## Figures

The figure sizes have been customised so that you can easily put two images side-by-side. 

```{r, fig.show='hold'}
plot(1:10)
plot(10:1)
```

You can enable figure captions by `fig_caption: yes` in YAML:

    output:
      rmarkdown::html_vignette:
        fig_caption: yes

Then you can use the chunk option `fig.cap = "Your figure caption."` in **knitr**.

## More Examples

You can write math expressions, e.g. $Y = X\beta + \epsilon$, footnotes^[A footnote here.], and tables, e.g. using `knitr::kable()`.

```{r, echo=FALSE, results='asis'}
knitr::kable(head(mtcars, 10))
```

Also a quote using `>`:

> "He who gives up [code] safety for [code] speed deserves neither."
([via](https://twitter.com/hadleywickham/status/504368538874703872))
